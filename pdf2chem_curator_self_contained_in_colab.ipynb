{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pdf2chem_curator.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "OP0GAVN_Xmq4",
        "OR4Mk87oX0IM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsAO7yruXlg_"
      },
      "source": [
        "# Introductory text and instructions\r\n",
        "\r\n",
        "The pdf2chem code extracts known chemicals mentioned in pdf files and outputs a csv with the machine readable SMILES format, making it easy to populate a chemical database from a set of pdfs.  If you curate multiple pdf files, this code will also output a csv file with all chemicals found in that session.\r\n",
        "___\r\n",
        "\r\n",
        "**Click the small folder icon** at the left of your screen.  That should connect this page to a hosted runtime and open up a file browser for you.  (At the time of this writing, it contains a folder called sample_data.  That's material from Google, and you can ignore it for this code.)\r\n",
        "\r\n",
        "Please **drag or otherwise upload the pdf(s) you'd like to curate into the hosted file browser at the left of your screen**.  If you need a pdf to try it with, [here's a link](https://pubs.acs.org/doi/pdf/10.1021/acs.jpcb.8b12322) to one that's open access.  Then, if you'd like your search to include 3-letter words, select \"exhaustive\" from the dropdown input below.  Note that this is likely to increase the number of false positive results, chemicals that weren't mentioned in the paper.\r\n",
        "\r\n",
        "Finally, go to the runtime menu and **click \"run all\"**.  If you **scroll down** to the last cell, you'll be able to watch the results trickle in as the program runs.  Note: the imports section takes a minute or two to download everything it needs before it starts in earnest.\r\n",
        "\r\n",
        "After it finishes, wait a few seconds, and csv files of the results will populate the file browser at the left.  You can then right-click them to download them to your computer.  If you change the dropdown choice or upload new files after a run, please click \"run all\" again to run the updated code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uR8s7cczycqo",
        "cellView": "form"
      },
      "source": [
        "#@title ## Please select the desired style of curation\n",
        "#@markdown The default style is \"regular\", which will ignore 3-letter words and result in fewer \"false positives,\" chemicals that were not actually mentioned in the pdf\n",
        "\n",
        "curation_type = \"Regular (ignores 3-letter words)\" #@param [\"Regular (ignores 3-letter words)\", \"Exhaustive (includes 3-letter words)\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP0GAVN_Xmq4"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VffJT09g92lX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ae86a44-0dc6-4aee-80ea-a99defc02397"
      },
      "source": [
        "!pip install chemdataextractor\r\n",
        "import chemdataextractor as cde\r\n",
        "!cde data download\r\n",
        "\r\n",
        "!pip install cirpy\r\n",
        "import cirpy\r\n",
        "import time\r\n",
        "\r\n",
        "import re\r\n",
        "import pandas as pd\r\n",
        "import os\r\n",
        "!pip install textract\r\n",
        "import textract\r\n",
        "\r\n",
        "from datetime import datetime\r\n",
        "\r\n",
        "if curation_type == \"Regular (ignores 3-letter words)\":\r\n",
        "  regex_number = 4\r\n",
        "else:\r\n",
        "  regex_number = 3\r\n",
        "\r\n",
        "import sys\r\n",
        "IN_COLAB = \"google.colab\" in sys.modules\r\n",
        "\r\n",
        "if IN_COLAB:\r\n",
        "  pdf_method = \"pdfminer\"\r\n",
        "else:\r\n",
        "  pdf_method = \"pdftotext\"\r\n",
        "\r\n",
        "print(\"We\\'ll use {} as the pdf extraction method.\".format(pdf_method))\r\n",
        "\r\n",
        "element_symbols = ['h', 'he', 'li', 'be', 'b', 'c', 'n', 'o', 'f', 'ne', 'na', 'mg', 'al', 'si', 'p', 's', 'cl', 'ar', 'k', 'ca', 'sc', 'ti', 'v', 'cr', 'mn', 'fe', 'co', 'ni', 'cu', 'zn', 'ga', 'ge', 'as', 'se', 'br', 'kr', 'rb', 'sr', 'y', 'zr', 'nb', 'mo', 'tc', 'ru', 'rh', 'pd', 'ag', 'cd', 'in', 'sn', 'sb', 'te', 'i', 'xe', 'cs', 'ba', 'la', 'ce', 'pr', 'nd', 'pm', 'sm', 'eu', 'gd', 'tb', 'dy', 'ho', 'er', 'tm', 'yb', 'lu', 'hf', 'ta', 'w', 're', 'os', 'ir', 'pt', 'au', 'hg', 'tl', 'pb', 'bi', 'po', 'at', 'rn', 'fr', 'ra', 'ac', 'th', 'pa', 'u', 'np', 'pu', 'am', 'cm', 'bk', 'cf', 'es', 'fm', 'md', 'no', 'lr', 'rf', 'db', 'sg', 'bh', 'hs', 'mt', 'ds ', 'rg ', 'cn ', 'nh', 'fl', 'mc', 'lv', 'ts', 'og']\r\n",
        "\r\n",
        "false_positives = ['reno', 'lower', 'format', 'lead', 'nci', 'cc', 'isi', 'doi', \"\\\\'b\", 'is', 'ph', 'mv', 'zone', 'based', 'on', 'final', 'kato', 'cm', 'life', 'versus', 'www', 'can', 'ate', 'mm', 'crystal', 'sem', 'an', 's1', 'force', 'may', 'any', 'lau', 'voltage', 'kc', 'mino', 'm. h.', 'set', 'selective', 'c.p.k.', 'same', 'page 10', 'm-1', 'ai', 'c1', 'm2', 'et', 'fulfill', 'dry', 'via', 'may', 'pka', 'any', 'edge', 'b.v.', 'final', 'rt', '2b', 'h.y.', 'y.k.', 'v.v.', 'w.y.', 'good', 'region', 'cycle', 'des', 'force', 'may', 'dsc', 'chcl', 'counter', 'van', 'see', 'best', 'green', 'equal', 'result', 'challenge', 'substance', 'spectrum', 'der', 'its', 'glass', 'all', 'new', 'mix', 'so', 'soc.', 'arm', 'nm', 'ran', 'enable', 'sd', 'saa', 'map', 'ac1', 'fab', 'act', 'b7', 'liu', 'check', 'dual', 'via', 'den', 'fc', 'if', 'rapid', 'san', 'van', 'control', 'see', 'harry', 'adam', 'line', 'ac-1', 'sig', 'recruit', 'bli', 'test', 'tau', 'acs', 'iap', 'box', 'campaign', 'target', 'gfp', 'new', 'cv', 'rt', 'lid', 'compound', 'selective', 'rfb', 'ment', 'est', 'mm', 'con', 'con-', 's4', 'harry', 'ip', 'lp', 'ple', 'ml', 'prone', 'pka', 'sum', 'derivative', 'ten', 'min', 'vortex', 'gradual', 'tot', 'ber', 'red', 'ing', 'para', 'phs', 'gen', 'dft', 'nals', 'enable', 'set', 'versus', 'ma', 'the', 'and', 'eo', 'cps', 'ep', 'are', 'same', 'cos', 'age', 'sem', 's4', 'cycle', 'far', 'cal', 'overall', 'net', 'et', 'ml', 's1', 'prone', 'capture', 'or', 'rise', 'but', 'diurnal', 'dry', 'may', 'of', 'off', 'dp', 'if', 'dants', 'van', 'eden', 'line', 'tx', 'top', 'va', 'per', 'ny', 'on', 'ing', 'cp', 'for', 'dc', 'air', 'nhe', 'gas', 'zonal', 'all', 'new', 'based', 'had', 'ph', 'cm3', 'pyrite', 'soc', 'ser', 'acc', 'res', 'eds', 'mp', 'pro', 'inc', 'im', 'bv', 'disodium', 'ab', 'ed', 'carboxylate', '1mm', 'nat', 'eq', 'acc', 'sci', 'mol', 'int', 'sc-s', 'scs', 'gu', 'atm', 'shi', '2az', 'abbott', 'ms', 'wang', 'pdc', 'franklin', 'bay', 'dess', 'hbd', 'retard', 'intercept', 'iii', 'acid', 'fraction', 'aldrich', 'triton', 'cda', 'cyano', 'vinyl', 'flux', 'ethyl', 'methyl', 'mit', 'trigger', 'accelerate', 'ants', 'pentyl', 'laser', 'india', 'dos', 'los', 'acetyl', 'dec', 'sheets', 'tem', 'dimethyl', 'serial', 'tag', 'tandem', 'trap', 'mic', 'exciton', 'aldehyde', 'combat', 'roi', 'probiotic', 'antiviral', 'cada', 'beam', 'austin', 'lactone', 'lumen', 'diethyl', 'optimal', 'sulfoxide', 'gm3', 'gel', 'blockade', 'omega', 'cubes', 'bin', 'alcohols', 'alcohol', 'benchmark', 'portal', 'matrix', 'apex', 'bacterial', 'cube', 'linker', 'cascade', 'optimum', 'carbonyl oxygen', 'facet', 'shield']\r\n",
        "\r\n",
        "if regex_number == 3:\r\n",
        "  false_positives = [word for word in false_positives if not re.search(\"[a-zA-Z0-9+-]{3}\", word) or re.search(\"[a-zA-Z0-9+-]{4}\", word)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting chemdataextractor\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/d7/2f33c7fab8bea8c107923541e447d481b7dd6309654244de91fb0e878e01/ChemDataExtractor-1.3.0-py3-none-any.whl (182kB)\n",
            "\r\u001b[K     |█▉                              | 10kB 12.1MB/s eta 0:00:01\r\u001b[K     |███▋                            | 20kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 30kB 9.9MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 40kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 51kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 61kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 71kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 81kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 92kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 102kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 112kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 122kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 133kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 143kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 153kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 163kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 174kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from chemdataextractor) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from chemdataextractor) (2.23.0)\n",
            "Collecting python-crfsuite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/47/58f16c46506139f17de4630dbcfb877ce41a6355a1bbf3c443edb9708429/python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 25.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from chemdataextractor) (1.4.4)\n",
            "Collecting pdfminer.six\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/f3/4fec7dabe8802ebec46141345bf714cd1fc7d93cb74ddde917e4b6d97d88/pdfminer.six-20201018-py3-none-any.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 33.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from chemdataextractor) (3.13)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from chemdataextractor) (7.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from chemdataextractor) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from chemdataextractor) (2.8.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from chemdataextractor) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from chemdataextractor) (4.2.6)\n",
            "Collecting DAWG\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/ef/91b619a399685f7a0a95a03628006ba814d96293bbbbed234ee66fbdefd9/DAWG-0.8.0.tar.gz (371kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 35.7MB/s \n",
            "\u001b[?25hCollecting cssselect\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->chemdataextractor) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->chemdataextractor) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->chemdataextractor) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->chemdataextractor) (2020.12.5)\n",
            "Collecting cryptography\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/1f/acde6ff69864c5e78b56488e3afd93c1ccc8c2651186e2a5f93d93f64859/cryptography-3.4.6-cp36-abi3-manylinux2014_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 29.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->chemdataextractor) (2.3.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->chemdataextractor) (1.14.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->chemdataextractor) (2.20)\n",
            "Building wheels for collected packages: DAWG\n",
            "  Building wheel for DAWG (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for DAWG: filename=DAWG-0.8.0-cp37-cp37m-linux_x86_64.whl size=857941 sha256=a54d2bd0f83e53d28e7abbf0159cb2907fc33b4b4bcc95ab254507bfa8a2cfa1\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/1f/f0/a5b1f9d02e193c997d252c33d215f24dfd7a448bc0166b2a12\n",
            "Successfully built DAWG\n",
            "Installing collected packages: python-crfsuite, cryptography, pdfminer.six, DAWG, cssselect, chemdataextractor\n",
            "Successfully installed DAWG-0.8.0 chemdataextractor-1.3.0 cryptography-3.4.6 cssselect-1.1.0 pdfminer.six-20201018 python-crfsuite-0.9.7\n",
            "INFO:chemdataextractor.data:Downloading http://data.chemdataextractor.org/models/cem_crf-1.0.pickle to /root/.local/share/ChemDataExtractor/models/cem_crf-1.0.pickle\n",
            "INFO:chemdataextractor.data:Downloading http://data.chemdataextractor.org/models/cem_crf_chemdner_cemp-1.0.pickle to /root/.local/share/ChemDataExtractor/models/cem_crf_chemdner_cemp-1.0.pickle\n",
            "INFO:chemdataextractor.data:Downloading http://data.chemdataextractor.org/models/cem_dict_cs-1.0.pickle to /root/.local/share/ChemDataExtractor/models/cem_dict_cs-1.0.pickle\n",
            "INFO:chemdataextractor.data:Downloading http://data.chemdataextractor.org/models/cem_dict-1.0.pickle to /root/.local/share/ChemDataExtractor/models/cem_dict-1.0.pickle\n",
            "INFO:chemdataextractor.data:Downloading http://data.chemdataextractor.org/models/clusters_chem1500-1.0.pickle to /root/.local/share/ChemDataExtractor/models/clusters_chem1500-1.0.pickle\n",
            "INFO:chemdataextractor.data:Downloading http://data.chemdataextractor.org/models/pos_ap_genia_nocluster-1.0.pickle to /root/.local/share/ChemDataExtractor/models/pos_ap_genia_nocluster-1.0.pickle\n",
            "INFO:chemdataextractor.data:Downloading http://data.chemdataextractor.org/models/pos_ap_genia-1.0.pickle to /root/.local/share/ChemDataExtractor/models/pos_ap_genia-1.0.pickle\n",
            "INFO:chemdataextractor.data:Downloading http://data.chemdataextractor.org/models/pos_ap_wsj_genia_nocluster-1.0.pickle to /root/.local/share/ChemDataExtractor/models/pos_ap_wsj_genia_nocluster-1.0.pickle\n",
            "INFO:chemdataextractor.data:Downloading http://data.chemdataextractor.org/models/pos_ap_wsj_genia-1.0.pickle to /root/.local/share/ChemDataExtractor/models/pos_ap_wsj_genia-1.0.pickle\n",
            "INFO:chemdataextractor.data:Downloading http://data.chemdataextractor.org/models/pos_ap_wsj_nocluster-1.0.pickle to /root/.local/share/ChemDataExtractor/models/pos_ap_wsj_nocluster-1.0.pickle\n",
            "INFO:chemdataextractor.data:Downloading http://data.chemdataextractor.org/models/pos_ap_wsj-1.0.pickle to /root/.local/share/ChemDataExtractor/models/pos_ap_wsj-1.0.pickle\n",
            "INFO:chemdataextractor.data:Downloading http://data.chemdataextractor.org/models/pos_crf_genia_nocluster-1.0.pickle to /root/.local/share/ChemDataExtractor/models/pos_crf_genia_nocluster-1.0.pickle\n",
            "INFO:chemdataextractor.data:Downloading http://data.chemdataextractor.org/models/pos_crf_genia-1.0.pickle to /root/.local/share/ChemDataExtractor/models/pos_crf_genia-1.0.pickle\n",
            "INFO:chemdataextractor.data:Downloading http://data.chemdataextractor.org/models/pos_crf_wsj_genia_nocluster-1.0.pickle to /root/.local/share/ChemDataExtractor/models/pos_crf_wsj_genia_nocluster-1.0.pickle\n",
            "INFO:chemdataextractor.data:Downloading http://data.chemdataextractor.org/models/pos_crf_wsj_genia-1.0.pickle to /root/.local/share/ChemDataExtractor/models/pos_crf_wsj_genia-1.0.pickle\n",
            "INFO:chemdataextractor.data:Downloading http://data.chemdataextractor.org/models/pos_crf_wsj_nocluster-1.0.pickle to /root/.local/share/ChemDataExtractor/models/pos_crf_wsj_nocluster-1.0.pickle\n",
            "INFO:chemdataextractor.data:Downloading http://data.chemdataextractor.org/models/pos_crf_wsj-1.0.pickle to /root/.local/share/ChemDataExtractor/models/pos_crf_wsj-1.0.pickle\n",
            "INFO:chemdataextractor.data:Downloading http://data.chemdataextractor.org/models/punkt_chem-1.0.pickle to /root/.local/share/ChemDataExtractor/models/punkt_chem-1.0.pickle\n",
            "Successfully downloaded 18 new data packages (0 existing)\n",
            "Collecting cirpy\n",
            "  Downloading https://files.pythonhosted.org/packages/fc/bc/3d7cb58ba6ffcde0d18d3f0e72a22caffe7741485bd1d286fd10d6a9a397/CIRpy-1.0.2.tar.gz\n",
            "Building wheels for collected packages: cirpy\n",
            "  Building wheel for cirpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cirpy: filename=CIRpy-1.0.2-cp37-none-any.whl size=7269 sha256=b81506117da1d81ed6a2261e91bd08e75ba4ae6615524c38e9f793734a76ae06\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/31/65/5c9e5a9c65c11466a31aa8360a146cb92ee3abbd628f2801f7\n",
            "Successfully built cirpy\n",
            "Installing collected packages: cirpy\n",
            "Successfully installed cirpy-1.0.2\n",
            "Collecting textract\n",
            "  Downloading https://files.pythonhosted.org/packages/32/31/ef9451e6e48a1a57e337c5f20d4ef58c1a13d91560d2574c738b1320bb8d/textract-1.6.3-py3-none-any.whl\n",
            "Collecting EbookLib==0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/38/7d6ab2e569a9165249619d73b7bc6be0e713a899a3bc2513814b6598a84c/EbookLib-0.17.1.tar.gz (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 7.5MB/s \n",
            "\u001b[?25hCollecting SpeechRecognition==3.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/e1/7f5678cd94ec1234269d23756dbdaa4c8cfaed973412f88ae8adf7893a50/SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8MB 132kB/s \n",
            "\u001b[?25hCollecting extract-msg==0.23.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/90/84485a914ed90adb5e87df17e626be04162fbba146dfecf34643659a4633/extract_msg-0.23.1-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.7MB/s \n",
            "\u001b[?25hCollecting argcomplete==1.10.0\n",
            "  Downloading https://files.pythonhosted.org/packages/4d/82/f44c9661e479207348a979b1f6f063625d11dc4ca6256af053719bbb0124/argcomplete-1.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.7/dist-packages (from textract) (3.0.4)\n",
            "Collecting pdfminer.six==20181108\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/fd/6e8746e6965d1a7ea8e97253e3d79e625da5547e8f376f88de5d024bacb9/pdfminer.six-20181108-py2.py3-none-any.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 34.5MB/s \n",
            "\u001b[?25hCollecting python-pptx==0.6.18\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/86/eb979f7b0333ec769041aae36df8b9f1bd8bea5bbad44620663890dce561/python-pptx-0.6.18.tar.gz (8.9MB)\n",
            "\u001b[K     |████████████████████████████████| 8.9MB 27.5MB/s \n",
            "\u001b[?25hCollecting six==1.12.0\n",
            "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
            "Collecting beautifulsoup4==4.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/b7/34eec2fe5a49718944e215fde81288eec1fa04638aa3fb57c1c6cd0f98c3/beautifulsoup4-4.8.0-py3-none-any.whl (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 9.8MB/s \n",
            "\u001b[?25hCollecting docx2txt==0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/7d/60ee3f2b16d9bfdfa72e8599470a2c1a5b759cb113c6fe1006be28359327/docx2txt-0.8.tar.gz\n",
            "Collecting xlrd==1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/16/63576a1a001752e34bf8ea62e367997530dc553b689356b9879339cf45a4/xlrd-1.2.0-py2.py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 34.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from EbookLib==0.17.1->textract) (4.2.6)\n",
            "Collecting olefile==0.46\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/81/e1ac43c6b45b4c5f8d9352396a14144bba52c8fec72a80f425f6a4d653ad/olefile-0.46.zip (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 41.3MB/s \n",
            "\u001b[?25hCollecting imapclient==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/39/e1c2c2c6e2356ab6ea81fcfc0a74b044b311d6a91a45300811d9a6077ef7/IMAPClient-2.1.0-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tzlocal==1.5.1 in /usr/local/lib/python3.7/dist-packages (from extract-msg==0.23.1->textract) (1.5.1)\n",
            "Collecting pycryptodome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/16/9627ab0493894a11c68e46000dbcc82f578c8ff06bc2980dcd016aea9bd3/pycryptodome-3.10.1-cp35-abi3-manylinux2010_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 26.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20181108->textract) (2.3.0)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.7/dist-packages (from python-pptx==0.6.18->textract) (7.0.0)\n",
            "Collecting XlsxWriter>=0.5.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/41/bf1aae04932d1eaffee1fc5f8b38ca47bbbf07d765129539bc4bcce1ce0c/XlsxWriter-1.3.7-py2.py3-none-any.whl (144kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 37.3MB/s \n",
            "\u001b[?25hCollecting soupsieve>=1.2\n",
            "  Downloading https://files.pythonhosted.org/packages/41/e7/3617a4b988ed7744743fb0dbba5aa0a6e3f95a9557b43f8c4740d296b48a/soupsieve-2.2-py3-none-any.whl\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tzlocal==1.5.1->extract-msg==0.23.1->textract) (2018.9)\n",
            "Building wheels for collected packages: EbookLib, python-pptx, docx2txt, olefile\n",
            "  Building wheel for EbookLib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for EbookLib: filename=EbookLib-0.17.1-cp37-none-any.whl size=38165 sha256=797731fb120e514219b450463cf0e31a706a4d58f19ff99908796768d3a335b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/11/01/951369cbbf8f96878786a1f4da68bd7ac19a5d945b38e03d54\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-pptx: filename=python_pptx-0.6.18-cp37-none-any.whl size=275704 sha256=d60d92021422696fb426bbed074de98f2609ef2b79bd52a6bb90d9eb5cd0be80\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/1f/2c/29acca422b420a0b5210bd2cd7e9669804520d602d2462f20b\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-cp37-none-any.whl size=3963 sha256=a5995a60ac5287fbb42551a473531054185fab97a5d7610878d5ea72674502a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/1f/26/a051209bbb77fc6bcfae2bb7e01fa0ff941b82292ab084d596\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35416 sha256=08384681ee3e785f77cbafa5d0934ad0f114fb644c29bafbb1cc619d939c7016\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/f4/11/bc4166107c27f07fd7bba707ffcb439619197638a1ac986df3\n",
            "Successfully built EbookLib python-pptx docx2txt olefile\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-api-python-client 1.12.8 has requirement six<2dev,>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-api-core 1.26.1 has requirement six>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: six, EbookLib, SpeechRecognition, olefile, imapclient, extract-msg, argcomplete, pycryptodome, pdfminer.six, XlsxWriter, python-pptx, soupsieve, beautifulsoup4, docx2txt, xlrd, textract\n",
            "  Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Found existing installation: pdfminer.six 20201018\n",
            "    Uninstalling pdfminer.six-20201018:\n",
            "      Successfully uninstalled pdfminer.six-20201018\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "Successfully installed EbookLib-0.17.1 SpeechRecognition-3.8.1 XlsxWriter-1.3.7 argcomplete-1.10.0 beautifulsoup4-4.8.0 docx2txt-0.8 extract-msg-0.23.1 imapclient-2.1.0 olefile-0.46 pdfminer.six-20181108 pycryptodome-3.10.1 python-pptx-0.6.18 six-1.12.0 soupsieve-2.2 textract-1.6.3 xlrd-1.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "bs4",
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "We'll use pdfminer as the pdf extraction method.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR4Mk87oX0IM"
      },
      "source": [
        "# Define functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sJiR8yw_De_"
      },
      "source": [
        "def quick_curate(pdf_path, pdf_method, false_positives, regex_number):\r\n",
        "  \r\n",
        "  # extract the text from the pdf\r\n",
        "  # the pdf_method should adapt to both local and hosted runtime compatibility\r\n",
        "  text = textract.process(pdf_path, method=pdf_method)\r\n",
        "\r\n",
        "  # queue up and reset list used to process the paper\r\n",
        "  temp_word_list = []\r\n",
        "\r\n",
        "  # strip new line and other markup from pdf mining\r\n",
        "  text = str(text).replace(\"\\\\n-\", '').replace('\\-\\n', '').replace('\\-\\n-', '').replace('\\\\n', ' ').replace('\\n', ' ').replace('.', '')\r\n",
        "  text = str(text).replace('*', \"\").replace('ISSN', '').replace('NSF', '').replace('NIH', '').replace(\"b'\", '').replace(r\"\\r\", '')\r\n",
        "\r\n",
        "  # split by white spaces\r\n",
        "  temp_word_list = re.split(\"\\s+\", str(text))\r\n",
        "\r\n",
        "  # try to remove reference section by cutting off everything after the last mention of reference\r\n",
        "  ref = [i for i, w in enumerate(temp_word_list) if w.lower().startswith('reference')]\r\n",
        "  #print(ref)\r\n",
        "  try:\r\n",
        "    temp_word_list = temp_word_list[:(ref[-1])]\r\n",
        "  except:\r\n",
        "    pass\r\n",
        "\r\n",
        "  # reconnect any words that got hyphenated and cut off at the end of a column\r\n",
        "  for i, word in enumerate(temp_word_list):\r\n",
        "    if re.search('[-]+$', word):\r\n",
        "      temp_word_list[i] = word.replace('-', '') + temp_word_list[i+1]\r\n",
        "      del(temp_word_list[i+1])\r\n",
        "\r\n",
        "  print('The initial list for {} has {} words.'.format(pdf_path, len(temp_word_list)))\r\n",
        "\r\n",
        "  # reconstruct a text string from the cleaned list, as cde's NLP works on strings\r\n",
        "  cleaned_text = ''\r\n",
        "  for word in temp_word_list:\r\n",
        "    cleaned_text += word \r\n",
        "    cleaned_text += ' '\r\n",
        "\r\n",
        "  # have cde do NLP on the string and convert the results into a list of strings\r\n",
        "  doc = cde.Document(cleaned_text)\r\n",
        "  chemicals_all = [span for span in doc.cems]\r\n",
        "  chem_strings = [str(word).lower().replace('\\n', ' ') for word in chemicals_all]\r\n",
        "\r\n",
        "  # remove any blanks or null values\r\n",
        "  chem_strings = [word for word in chem_strings if word]\r\n",
        "\r\n",
        "  # remove anything left with a backslash in it\r\n",
        "  chem_strings = [word for word in chem_strings if not re.search('[\\\\\\+]', word)]\r\n",
        "\r\n",
        "  print('We\\'ll attempt to resolve {} potential chemicals.'.format(len(chem_strings)))\r\n",
        "\r\n",
        "  # reset lists used for processing query hits and misses\r\n",
        "  smiles_list = []\r\n",
        "  already_queried = []\r\n",
        "  missed_items = []\r\n",
        "\r\n",
        "  for item in chem_strings:\r\n",
        "\r\n",
        "    # keeping element symbols, such as H, C, or Na\r\n",
        "    # this may turn into an option\r\n",
        "    if item in element_symbols:\r\n",
        "      smiles_list.append(cirpy.resolve(item, 'smiles'))\r\n",
        "      print(item, smiles_list[-1])\r\n",
        "      continue\r\n",
        "\r\n",
        "    # adapt the regex code that leaves out short words/abbreviations to the user input above\r\n",
        "    if regex_number == 4:\r\n",
        "      \r\n",
        "      if not re.search(\"[a-zA-Z0-9+-]{4}\", item):\r\n",
        "        smiles_list.append(None)\r\n",
        "        print('Found a word that\\'s a likely false positive: {}'.format(item))\r\n",
        "        missed_items.append(item)\r\n",
        "        continue \r\n",
        "\r\n",
        "    if regex_number == 3:\r\n",
        "      \r\n",
        "      if not re.search(\"[a-zA-Z0-9+-]{3}\", item):\r\n",
        "        smiles_list.append(None)\r\n",
        "        print('Found a word that\\'s a likely false positive: {}'.format(item))\r\n",
        "        missed_items.append(item) \r\n",
        "        continue\r\n",
        "\r\n",
        "    # save time by not querying chemicals that are in the text many times\r\n",
        "    if item in already_queried:\r\n",
        "      smiles_list.append(None)\r\n",
        "      print('We\\'ve already queried this one: {}'.format(item)) \r\n",
        " \r\n",
        " \r\n",
        "    # don't query the chemical if it's a known false positive\r\n",
        "    # these include author names and a few other odds and ends\r\n",
        "    elif item.strip('.').strip(',').lower() in false_positives:\r\n",
        "      smiles_list.append(None)\r\n",
        "      print('Found one known to be a false positive: {}'.format(item))\r\n",
        "\r\n",
        "    # if the item passes all the tests, attempt to resolve it via NIH's CIR\r\n",
        "    else:\r\n",
        "      try:\r\n",
        "          smiles_list.append(cirpy.resolve(item, 'smiles'))\r\n",
        "          print(item, smiles_list[-1])\r\n",
        "          time.sleep(0.21)\r\n",
        "          \r\n",
        "      # except loop in here to account for internet stability issues and the like\r\n",
        "      except:\r\n",
        "          try: \r\n",
        "            print('Exception raised.  Pausing for 2 seconds and trying again')\r\n",
        "            time.sleep(2)\r\n",
        "            smiles_list.append(cirpy.resolve(item, 'smiles'))\r\n",
        "            print(smiles_list[-1]) \r\n",
        "          except:\r\n",
        "            try:\r\n",
        "              print('Exception raised.  Pausing for another 2 seconds and trying again')\r\n",
        "              time.sleep(2)\r\n",
        "              smiles_list.append(cirpy.resolve(item, 'smiles'))\r\n",
        "              print(smiles_list[-1])\r\n",
        "            except:\r\n",
        "              try:\r\n",
        "                print('Exception raised.  Pausing for one more stretch and trying again')\r\n",
        "                time.sleep(2)\r\n",
        "                smiles_list.append(cirpy.resolve(item, 'smiles'))\r\n",
        "                print(smiles_list[-1])   \r\n",
        "              except:\r\n",
        "                print('It still raised an exception.  Here\\'s how far it got:')\r\n",
        "                print(smiles_list)\r\n",
        "                print(len(smiles_list))\r\n",
        "                print('This item will be added to a list called missed items.')\r\n",
        "                print(item)\r\n",
        "                smiles_list.append('Check')\r\n",
        "                missed_items.append(item)\r\n",
        "    already_queried.append(item)\r\n",
        "\r\n",
        "  # tidy these up into pandas dataframes and export them as csv files\r\n",
        "  chem_df = pd.DataFrame(zip(chem_strings, smiles_list), columns=('Name', 'SMILES'))\r\n",
        "  chem_df = chem_df.dropna()\r\n",
        "  chem_df.to_csv(os.path.splitext(pdf_path)[0]+'_'+datetime.today().strftime('%Y%m%d')+'_names_and_SMILES.csv')\r\n",
        "  if missed_items:\r\n",
        "    missed_df = pd.DataFrame(missed_items, columns=['Missed'])\r\n",
        "    missed_df.to_csv(os.path.splitext(pdf_path)[0]+'_'+datetime.today().strftime('%Y%m%d')+'_zzz_missed_items.csv')\r\n",
        "\r\n",
        "  # return chem_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Gru4BXt12Vq"
      },
      "source": [
        "def aggregate_csv_files():\r\n",
        "  # combines all results files into a single csv file\r\n",
        "  all_chemicals = pd.concat([pd.read_csv(filename) for filename in os.listdir(pdf_dir) if re.search('csv$', filename)])\r\n",
        "  all_chemicals.to_csv(datetime.today().strftime('%Y%m%d')+\"combined_csv.csv\", index=False, encoding='utf-8-sig')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JawBfYyYFy4"
      },
      "source": [
        "# Curate pdfs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpbNWvqBFK91",
        "cellView": "form",
        "outputId": "808a2d63-f7b3-45e6-d211-4007622d177d"
      },
      "source": [
        "#@title ## Curator output will appear below\r\n",
        "\r\n",
        "#pdf_dir = '/content'\r\n",
        "pdf_dir = os.getcwd()\r\n",
        "\r\n",
        "pd.DataFrame(data=None, columns=('Name', 'SMILES'))\r\n",
        "\r\n",
        "assert os.path.exists(pdf_dir), \"I did not find the directory at, \"+str(pdf_dir)\r\n",
        "\r\n",
        "os.chdir(pdf_dir)\r\n",
        "\r\n",
        "for filename in os.listdir(pdf_dir):\r\n",
        "  if re.search('pdf$', filename):\r\n",
        "    try:\r\n",
        "      chemicals = quick_curate(filename, pdf_method, false_positives, regex_number)\r\n",
        "    except:\r\n",
        "      print('An exception was raised for ' + filename)\r\n",
        "      print('Most likely, an error occurred when trying to extract text from the pdf.')\r\n",
        "\r\n",
        "try:\r\n",
        "  aggregate_csv_files()\r\n",
        "except:\r\n",
        "  \"An error occurred while trying to combine the output csv files.\"    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The initial list for 20210309_short_doc_for_python-testing.pdf has 161 words.\n",
            "We'll attempt to resolve 2 potential chemicals.\n",
            "urea NC(N)=O\n",
            "choline chloride [Cl-].C[N+](C)(C)CCO\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}